ğŸ”‘ **PRT(Peer Review Template)**
ë¦¬ë·°ì–´: ê¹€ë‚˜ê²½

- [x]  **1. ì£¼ì–´ì§„ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì™„ì„±ëœ ì½”ë“œê°€ ì œì¶œë˜ì—ˆë‚˜ìš”? (ì™„ì„±ë„)**
    - ë¬¸ì œì—ì„œ ìš”êµ¬í•˜ëŠ” ìµœì¢… ê²°ê³¼ë¬¼ì´ ì²¨ë¶€ë˜ì—ˆëŠ”ì§€ í™•ì¸
    - ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì™„ì„±ëœ ì½”ë“œë€ í”„ë¡œì íŠ¸ ë£¨ë¸Œë¦­ 3ê°œ ì¤‘ 2ê°œ, 
    í€˜ìŠ¤íŠ¸ ë¬¸ì œ ìš”êµ¬ì¡°ê±´ ë“±ì„ ì§€ì¹­
        - í•´ë‹¹ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë¶€ë¶„ì˜ ì½”ë“œ ë° ê²°ê³¼ë¬¼ì„ ìº¡ì³í•˜ì—¬ ì‚¬ì§„ìœ¼ë¡œ ì²¨ë¶€
          1. í•œê¸€ ì½”í¼ìŠ¤ë¥¼ ê°€ê³µí•˜ì—¬ BERT pretrainìš© ë°ì´í„°ì…‹ì„ ì˜ ìƒì„±í•˜ì˜€ë‹¤.
             ```
             def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):
                """ pretrain ë°ì´í„° ìƒì„± """
                def save_pretrain_instances(out_f, doc):
                    instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)
                    for instance in instances:
                        out_f.write(json.dumps(instance, ensure_ascii=False))
                        out_f.write("\n")
            
                # íŠ¹ìˆ˜ë¬¸ì 7ê°œë¥¼ ì œì™¸í•œ vocab_list ìƒì„±
                vocab_list = []
                for id in range(7, len(vocab)):
                    if not vocab.is_unknown(id):
                        vocab_list.append(vocab.id_to_piece(id))
            
                # line count í™•ì¸
                line_cnt = 0
                with open(in_file, "r") as in_f:
                    for line in in_f:
                        line_cnt += 1
            
                with open(in_file, "r") as in_f:
                    with open(out_file, "w") as out_f:
                        doc = []
                        for line in tqdm(in_f, total=line_cnt):
                            line = line.strip()
                            if line == "":  # lineì´ ë¹ˆì¤„ ì¼ ê²½ìš° (ìƒˆë¡œìš´ ë‹¨ë½ì„ ì˜ë¯¸ í•¨)
                                if 0 < len(doc):
                                    save_pretrain_instances(out_f, doc)
                                    doc = []
                            else:  # lineì´ ë¹ˆì¤„ì´ ì•„ë‹ ê²½ìš° tokenize í•´ì„œ docì— ì €ì¥
                                pieces = vocab.encode_as_pieces(line)
                                if 0 < len(pieces):
                                    doc.append(pieces)
                        if 0 < len(doc):  # ë§ˆì§€ë§‰ì— ì²˜ë¦¬ë˜ì§€ ì•Šì€ docê°€ ìˆëŠ” ê²½ìš°
                            save_pretrain_instances(out_f, doc)
                            doc = []
              ```
             > `make_pretrain_data()` í•¨ìˆ˜ê°€ ì˜ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤
          2. êµ¬í˜„í•œ BERT ëª¨ë¸ì˜ í•™ìŠµì´ ì•ˆì •ì ìœ¼ë¡œ ì§„í–‰ë¨ì„ í™•ì¸í•˜ì˜€ë‹¤.
             ![image](https://github.com/user-attachments/assets/662d1376-2fdd-4d2f-b65c-4603dc1f36a1)
             > í…ŒìŠ¤íŠ¸ ì…ë ¥ì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµì´ ì •ìƒì ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤. 
          3. 1Mì§œë¦¬ mini BERT ëª¨ë¸ì˜ ì œì‘ê³¼ í•™ìŠµì´ ì •ìƒì ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆë‹¤.
             > ì‹¤ì œ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œëŠ” í•™ìŠµì´ ì§„í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤
- [x]  **2. í”„ë¡œì íŠ¸ì—ì„œ í•µì‹¬ì ì¸ ë¶€ë¶„ì— ëŒ€í•œ ì„¤ëª…ì´ ì£¼ì„(ë‹¥ìŠ¤íŠ¸ë§) ë° ë§ˆí¬ë‹¤ìš´ í˜•íƒœë¡œ ì˜ ê¸°ë¡ë˜ì–´ìˆë‚˜ìš”? (ì„¤ëª…)**
    - [ ]  ëª¨ë¸ ì„ ì • ì´ìœ 
    - [ ]  Metrics ì„ ì • ì´ìœ 
    - [ ]  Loss ì„ ì • ì´ìœ 
    ![image](https://github.com/user-attachments/assets/8bead9c5-6a60-4689-a4eb-244a91595678)
        > ë” ì°¾ì•„ë³¸ ë‚´ìš©ì— ëŒ€í•´ ìì„¸í•œ ì„¤ëª…ì´ ì²¨ë¶€ë˜ì–´ ìˆìŠµë‹ˆë‹¤


- [ ]  **3. ì²´í¬ë¦¬ìŠ¤íŠ¸ì— í•´ë‹¹í•˜ëŠ” í•­ëª©ë“¤ì„ ëª¨ë‘ ìˆ˜í–‰í•˜ì˜€ë‚˜ìš”? (ë¬¸ì œ í•´ê²°)**
    - [x]  ë°ì´í„°ë¥¼ ë¶„í• í•˜ì—¬ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í–ˆë‚˜ìš”? (train, validation, test ë°ì´í„°ë¡œ êµ¬ë¶„)
      ```
        def load_pre_train_data(vocab, filename, n_seq, count=None):
            """
            í•™ìŠµì— í•„ìš”í•œ ë°ì´í„°ë¥¼ ë¡œë“œ
            :param vocab: vocab
            :param filename: ì „ì²˜ë¦¬ëœ json íŒŒì¼
            :param n_seq: ì‹œí€€ìŠ¤ ê¸¸ì´ (number of sequence)
            :param count: ë°ì´í„° ìˆ˜ ì œí•œ (Noneì´ë©´ ì „ì²´)
            :return enc_tokens: encoder inputs
            :return segments: segment inputs
            :return labels_nsp: nsp labels
            :return labels_mlm: mlm labels
            """
            total = 0
            with open(filename, "r") as f:
                for line in f:
                    total += 1
                    # ë°ì´í„° ìˆ˜ ì œí•œ
                    if count is not None and count <= total:
                        break
            
            # np.memmapì„ ì‚¬ìš©í•˜ë©´ ë©”ëª¨ë¦¬ë¥¼ ì ì€ ë©”ëª¨ë¦¬ì—ì„œë„ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ê°€ ê°€ëŠ¥ í•¨
            enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))
            segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))
            labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))
            labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))
        
            with open(filename, "r") as f:
                for i, line in enumerate(tqdm(f, total=total)):
                    if total <= i:
                        print("data load early stop", total, i)
                        break
                    data = json.loads(line)
                    # encoder token
                    enc_token = [vocab.piece_to_id(p) for p in data["tokens"]]
                    enc_token += [0] * (n_seq - len(enc_token))
                    # segment
                    segment = data["segment"]
                    segment += [0] * (n_seq - len(segment))
                    # nsp label
                    label_nsp = data["is_next"]
                    # mlm label
                    mask_idx = np.array(data["mask_idx"], dtype=np.int)
                    mask_label = np.array([vocab.piece_to_id(p) for p in data["mask_label"]], dtype=np.int)
                    label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)
                    label_mlm[mask_idx] = mask_label
        
                    assert len(enc_token) == len(segment) == len(label_mlm) == n_seq
        
                    enc_tokens[i] = enc_token
                    segments[i] = segment
                    labels_nsp[i] = label_nsp
                    labels_mlm[i] = label_mlm
        
            return (enc_tokens, segments), (labels_nsp, labels_mlm)
      ```
      > mlm, nsp ê° taskì— ì•Œë§ê²Œ ë°ì´í„°ì…‹ì´ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤
    - [ ]  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë³€ê²½í•´ê°€ë©° ì—¬ëŸ¬ ì‹œë„ë¥¼ í–ˆë‚˜ìš”? (learning rate, dropout rate, unit, batch size, epoch ë“±)
    - [ ]  ê° ì‹¤í—˜ì„ ì‹œê°í™”í•˜ì—¬ ë¹„êµí•˜ì˜€ë‚˜ìš”?
    - [ ]  ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ê°€ ê¸°ë¡ë˜ì—ˆë‚˜ìš”?

- [x]  **4. í”„ë¡œì íŠ¸ì— ëŒ€í•œ íšŒê³ ê°€ ìƒì„¸íˆ ê¸°ë¡ ë˜ì–´ ìˆë‚˜ìš”? (íšŒê³ , ì •ë¦¬)**
    - [x]  ë°°ìš´ ì 
    - [x]  ì•„ì‰¬ìš´ ì 
    - [x]  ëŠë‚€ ì 
    - [x]  ì–´ë ¤ì› ë˜ ì 
          ![image](https://github.com/user-attachments/assets/0ba4ac23-9480-4252-b250-856a13151652)

